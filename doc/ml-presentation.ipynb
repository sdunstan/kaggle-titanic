{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Math Is Fun!!!!!!\n",
    "Linear algebra and calculus are used to blah blah the blah blah so you can blah blah machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![now for something completely different](completely-different.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why oh why would do I do this?\n",
    "The problems machine learning solves are real pain in the neck problems. When you have some problem that looks like it is going to take one billion if-then-else statements, use machine learning to solve it.\n",
    "\n",
    "__A Definition__\n",
    "\n",
    "__Artificial intelligence:__ _a computer program used to solve an intractable problem._\n",
    "\n",
    "__An Example__\n",
    "\n",
    "The \"hello world\" of Kaggle is the morose task of figuring out who dies in the fridgid waters of the north atlantic when the Titanic tragically collides with an iceberg and sinks to the murky depths.\n",
    "\n",
    "SPOILER ALERT! Kate Winslet survives but Leonardo DiCaprio is turned into a dead human popcicle and sinks beneath the whitecaps. Can we generalize this idea? It seemed like women and children were piled onto the lifeboats while men and cello players did not fare as well. Is it cynical to guess that rich people survive and steerage class die? Let's use MACHINE LEARNING to test the hypothesis that sex, age, and ticket price determines if a passenger survives the Titanic disaster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUT FIRST, DATA SCIENCE IS MUNGING!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's load the data and take a look...\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "data = pd.read_csv('../input/train.csv')\n",
    "print(data[:5]) # print out first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Hmmm, let's focus on Sex, Age, and what they paid for their ticket\n",
    "print(data[['Survived', 'Sex', 'Age', 'Fare']][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three problems with this data:\n",
    "\n",
    "1. It all must be numeric data for our ML maths to crunch it.\n",
    "2. That NaN there is going to cause all kinds of trouble.\n",
    "3. (Just trust me) all the data needs to be on the same scale.\n",
    "\n",
    "The solutions for these problems:\n",
    "\n",
    "1. Convert male/female to 0 or 1\n",
    "2. Make some default for missing NaN data\n",
    "3. \"<a href=\"https://en.wikipedia.org/wiki/Normalization_(statistics)\">Normalize</a>\" the data. We will use feature scaling in this case to jam everything in to the range [0-1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No snickering\n",
    "def sex_conversion(row):\n",
    "    if row['Sex'] == 'female':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "data1 = data.copy()\n",
    "data1['norm_sex'] = data1.apply(sex_conversion, axis=1)\n",
    "\n",
    "print(data1[['Survived', 'Sex', 'norm_sex', 'Age', 'Fare']][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing data with the median. Other strategies include:\n",
    "#  Remove the row or use the most frequently occurring value.\n",
    "#  Interestingly, we could create another ML model to predict a row's missing\n",
    "#  values and fill in with that!\n",
    "data2 = data1.fillna(data.median())\n",
    "print(data2[['Survived', 'norm_sex', 'Age', 'Fare']][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_scaler(df, col):\n",
    "    df['norm_' + col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min())\n",
    "    \n",
    "data3 = data2.copy()\n",
    "feature_scaler(data3, 'Age')\n",
    "feature_scaler(data3, 'Fare')\n",
    "print(data3[['Survived', 'norm_sex', 'Age', 'norm_Age', 'Fare', 'norm_Fare']][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Great!__ Our data is now all munged into submission.\n",
    "\n",
    "The next step is to extract our survivor flag into an array (vector). This is the \"label\" for our supervised machine learning problem.\n",
    "\n",
    "Then, we need to split our input features and labels into two groups: one for training and one for testing.\n",
    "\n",
    "There are nice utilities for these tasks. Also, we will start calling our data X and y since they are our dependent and independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = np.array(data3['Survived'].values).T\n",
    "X = data3[['norm_sex', 'norm_Age', 'norm_Fare']].values # we have three features\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n",
    "print(y[:10])\n",
    "print(X[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ...and now presenting Keras\n",
    "Keras is a neural network API that abstracts low-level toolkits like Theano and TensorFlow. It makes it really easy to read the code and abstracts away a lot of boilerplate. If you think of a neural network as something like this:\n",
    "\n",
    "![Neural Network](Colored_neural_network.svg)\n",
    "\n",
    "Using Keras will take your intuition and turn it into code very easily.\n",
    "\n",
    "The basic idea is that we define a neural network architecture, compile it down to the underlying implementation, and then fit the model with our training data. Once that is done, we can use the model to predict new observations.\n",
    "\n",
    "Let's code it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "def create_model(input_params):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(150, input_dim=input_params, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    sgd = optimizers.SGD(lr=0.0005, clipnorm=1., momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['binary_accuracy', 'accuracy'])\n",
    "    return model\n",
    "    \n",
    "baseline_model = create_model(3)\n",
    "baseline_model.summary()\n",
    "\n",
    "history = baseline_model.fit(X_train, y_train, batch_size=25, epochs=35, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['binary_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.round(baseline_model.predict(X_test, verbose=0))\n",
    "\n",
    "def evaluate(prediction, y_test):\n",
    "    correct = 0\n",
    "    for i in range(0, prediction.shape[0]):\n",
    "    #     print(\"%f == %f ?\" % (prediction[i], y_test[i]))\n",
    "        if prediction[i] == y_test[i]:\n",
    "            correct = correct + 1\n",
    "\n",
    "    accuracy = correct / prediction.shape[0]\n",
    "\n",
    "    print(\"\\nDONE. %f\" % accuracy)\n",
    "    \n",
    "evaluate(prediction, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. Maybe about 81%? Better than random guessing.\n",
    "\n",
    "Let's put that in terms of our original hypothesis. We can say that given sex, age, and ticket price our model can correctly predict Titanic disaster survival 81% of the time.\n",
    "\n",
    "Next, let's add a few more input features to see if that helps..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a lot of nulls for the Cabin, let's use this for the feature\n",
    "data4 = data3.copy()\n",
    "\n",
    "data4.loc[pd.notnull(data4.Cabin), 'Cabin'] = 1.0\n",
    "data4.loc[pd.isnull(data4.Cabin), 'Cabin'] = 0.0\n",
    "data4[['Survived', 'Cabin', 'PassengerId']].groupby(['Survived','Cabin']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embarked_conversion(row):\n",
    "    if row['Embarked'] == 'C':\n",
    "        return 1\n",
    "    elif row['Embarked'] == 'Q':\n",
    "        return 2\n",
    "    elif row['Embarked'] == 'S':\n",
    "        return 3\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "data4['norm_embarked'] = data1.apply(embarked_conversion, axis=1)\n",
    "data4[['Survived', 'norm_embarked']][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "feature_scaler(data4, 'SibSp')\n",
    "feature_scaler(data4, 'Parch')\n",
    "feature_scaler(data4, 'Pclass')\n",
    "X = data4[['norm_sex', 'norm_Age', 'norm_Fare', 'SibSp', 'Parch', 'Cabin', 'Pclass', 'norm_embarked']].values\n",
    "print(X[:3])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='/tmp/titanic-weights.hdf5', verbose=0, save_best_only=True)\n",
    "\n",
    "all_features_model = Sequential()\n",
    "all_features_model.add(Dense(100, input_dim=8, activation='relu'))\n",
    "all_features_model.add(Dropout(0.2))\n",
    "all_features_model.add(Dense(50, activation='relu'))\n",
    "all_features_model.add(Dropout(0.2))\n",
    "all_features_model.add(Dense(1, activation='sigmoid'))\n",
    "all_features_model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['binary_accuracy', 'accuracy'])\n",
    "\n",
    "history = all_features_model.fit(X_train, y_train, batch_size=10, epochs=100, validation_data=(X_test, y_test), callbacks=[checkpointer], verbose=0)\n",
    "\n",
    "plt.plot(history.history['binary_accuracy'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "\n",
    "all_features_model.load_weights('/tmp/titanic-weights.hdf5')\n",
    "\n",
    "all_features_prediction = np.round(all_features_model.predict(X_test, verbose=0))\n",
    "evaluate(all_features_prediction, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "evalData = pd.read_csv('../input/test.csv')\n",
    "\n",
    "evalData['norm_embarked'] = evalData.apply(embarked_conversion, axis=1)\n",
    "evalData['norm_sex'] = evalData.apply(sex_conversion, axis=1)\n",
    "feature_scaler(evalData, 'Age')\n",
    "feature_scaler(evalData, 'Fare')\n",
    "evalData.loc[pd.notnull(evalData.Cabin), 'Cabin'] = 1.0\n",
    "evalData.loc[pd.isnull(evalData.Cabin), 'Cabin'] = 0.0\n",
    "\n",
    "X_eval = evalData[['norm_sex', 'norm_Age', 'norm_Fare', 'SibSp', 'Parch', 'Cabin', 'Pclass', 'norm_embarked']].values\n",
    "print(X_eval[:5])\n",
    "\n",
    "evalPrediction = np.round(all_features_model.predict(X_eval, verbose=0)).astype(int)\n",
    "\n",
    "print(evalPrediction[0,:10])\n",
    "\n",
    "evalDataFrame = DataFrame.from_records(evalData, 'PassengerId')\n",
    "evalDataFrame['Survived'] = evalPrediction\n",
    "\n",
    "print(evalDataFrame[['Survived']])\n",
    "\n",
    "evalDataFrame.to_csv('prediction2.csv', columns=['Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
